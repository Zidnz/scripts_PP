{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c4f16ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta.pip_utils import configure_spark_with_delta_pip \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.functions import col,when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71c9eedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/17 15:13:10 WARN Utils: Your hostname, Zidnz, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/11/17 15:13:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/zidnz/mi_proyecto_env/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/zidnz/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/zidnz/.ivy2.5.2/jars\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-061a92b2-98c7-46d2-9d57-f43bdef29ad1;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.13;4.0.0 in central\n",
      "\tfound io.delta#delta-storage;4.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in central\n",
      ":: resolution report :: resolve 221ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.13;4.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-061a92b2-98c7-46d2-9d57-f43bdef29ad1\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/9ms)\n",
      "25/11/17 15:13:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "builder = SparkSession.builder \\\n",
    "    .appName(\"EntrenarModeloFraude\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbad4239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo Delta Lake (con features) desde: /home/zidnz/DanaPP/proyecto_fraude/delta_lake_features\n",
      "¡Tabla de features lista para el modelo!\n",
      "root\n",
      " |-- tipo_transaccion: string (nullable = true)\n",
      " |-- id_transaccion: integer (nullable = true)\n",
      " |-- id_cliente: integer (nullable = true)\n",
      " |-- nombre_cliente: string (nullable = true)\n",
      " |-- cuenta_origen: double (nullable = true)\n",
      " |-- fecha: string (nullable = true)\n",
      " |-- monto: double (nullable = true)\n",
      " |-- divisa: string (nullable = true)\n",
      " |-- ciudad: string (nullable = true)\n",
      " |-- cuenta_destino: double (nullable = true)\n",
      " |-- categoria: string (nullable = true)\n",
      " |-- hora_movimiento: timestamp (nullable = true)\n",
      " |-- concepto_movimiento: string (nullable = true)\n",
      " |-- saldo_previo: double (nullable = true)\n",
      " |-- saldo_posterior: double (nullable = true)\n",
      " |-- canal_transaccion: string (nullable = true)\n",
      " |-- medio_pago: string (nullable = true)\n",
      " |-- estatus: string (nullable = true)\n",
      " |-- fraude_probable: string (nullable = true)\n",
      " |-- es_fraude: integer (nullable = true)\n",
      " |-- fecha_dt: date (nullable = true)\n",
      " |-- hora_del_dia: integer (nullable = true)\n",
      " |-- feat_horario_riesgo: integer (nullable = true)\n",
      " |-- feat_tipo_riesgo: integer (nullable = true)\n",
      " |-- feat_canal_riesgo: integer (nullable = true)\n",
      " |-- feat_perfil_riesgo_completo: integer (nullable = true)\n",
      " |-- feat_log_monto: double (nullable = true)\n",
      " |-- monto_promedio_tipo: double (nullable = true)\n",
      " |-- feat_ratio_monto_vs_tipo: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DELTA_PATH = \"/home/zidnz/DanaPP/proyecto_fraude/delta_lake_features\"\n",
    "print(f\"Leyendo Delta Lake (con features) desde: {DELTA_PATH}\")\n",
    "\n",
    "df_modelo_input = spark.read.format(\"delta\").load(DELTA_PATH)\n",
    "\n",
    "print(\"¡Tabla de features lista para el modelo!\")\n",
    "df_modelo_input.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de53d862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PASO 1: Preparar el DataFrame para el Pipeline ===\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# El pipeline de ML de Spark necesita que la columna objetivo se llame \"label\"\n",
    "df_modelo_input = df_modelo_input.withColumnRenamed(\"es_fraude\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "254304d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PASO 2: Definir las Columnas ===\n",
    "\n",
    "# Tus features numéricas (las que ya creaste)\n",
    "columnas_numericas = [\n",
    "    \"feat_horario_riesgo\",\n",
    "    \"feat_tipo_riesgo\",\n",
    "    \"feat_canal_riesgo\",\n",
    "    \"feat_perfil_riesgo_completo\",\n",
    "    \"feat_log_monto\",\n",
    "    \"feat_ratio_monto_vs_tipo\",\n",
    "    \"monto\" # Incluir el monto original también es útil\n",
    "]\n",
    "\n",
    "# Tus features categóricas (las originales que el modelo también usará)\n",
    "columnas_categoricas = [\n",
    "    \"tipo_transaccion\", \n",
    "    \"canal_transaccion\", \n",
    "    \"categoria\", \n",
    "    \"divisa\", \n",
    "    \"estatus\", \n",
    "    \"medio_pago\", \n",
    "    \"ciudad\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "905a4d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PASO 3: Crear los \"Stages\" del Pipeline ===\n",
    "\n",
    "# Stage 1: Convertir strings categóricos a índices numéricos\n",
    "# (Ej: \"App movil\" -> 0.0, \"Web\" -> 1.0)\n",
    "indexer = StringIndexer(\n",
    "    inputCols=columnas_categoricas, \n",
    "    outputCols=[c + \"_idx\" for c in columnas_categoricas],\n",
    "    handleInvalid=\"keep\" # Si aparece una categoría nueva, la agrupa\n",
    ")\n",
    "\n",
    "# Stage 2: Aplicar One-Hot Encoding a esos índices\n",
    "# (Convierte \"0.0\" en un vector [1, 0, 0])\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=indexer.getOutputCols(),\n",
    "    outputCols=[c + \"_ohe\" for c in columnas_categoricas]\n",
    ")\n",
    "\n",
    "# Stage 3: Ensamblar TODO en un solo vector de features\n",
    "# (Junta tus features numéricas + las nuevas categóricas codificadas)\n",
    "features_list = columnas_numericas + encoder.getOutputCols()\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=features_list,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# Stage 4: El Modelo (Usaremos un Random Forest)\n",
    "# Es robusto, bueno para empezar y maneja bien el desbalance.\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# 97.46% (194921) No-Fraude vs 2.54% (5079) Fraude\n",
    "# Peso = Total No-Fraude / Total Fraude = 194921 / 5079 = 38.38\n",
    "ratio_peso = 194921 / 5079\n",
    "\n",
    "df_modelo_ponderado = df_modelo_input.withColumn(\"classWeight\",\n",
    "    when(col(\"label\") == 1, ratio_peso).otherwise(1.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23041a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PASO 4: Crear el Pipeline ===\n",
    "# (Definir indexer, encoder, assembler... igual que antes)\n",
    "# ...\n",
    "\n",
    "# MEJORA: Decirle al modelo que use la columna de peso\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    weightCol=\"classWeight\",  # Usamos la ponderación\n",
    "    numTrees=100,\n",
    "    maxDepth=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[indexer, encoder, assembler, rf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46bbd69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de entrenamiento (para CV): 160255\n",
      "Datos de prueba (finales): 39745\n"
     ]
    }
   ],
   "source": [
    "# === PASO 5: Dividir Datos ===\n",
    "# Usamos el DataFrame ponderado\n",
    "(train_data, test_data) = df_modelo_ponderado.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Datos de entrenamiento (para CV): {train_data.count()}\")\n",
    "print(f\"Datos de prueba (finales): {test_data.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4302ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PASO 6: Definir el Evaluador y el CrossValidator ===\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    labelCol=\"label\",\n",
    "    metricName=\"areaUnderPR\"\n",
    ")\n",
    "\n",
    "# --- ESTA ES LA CORRECCIÓN ---\n",
    "# 1. Crear una parrilla de parámetros (aunque esté vacía)\n",
    "paramGrid = ParamGridBuilder().build() \n",
    "\n",
    "# 2. Definir el CrossValidator CON la parrilla\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,  # <-- ESTA LÍNEA ES LA QUE FALTA Y ARREGLA EL ERROR\n",
    "    evaluator=evaluator,\n",
    "    numFolds=5,\n",
    "    seed=42\n",
    ")\n",
    "# --- FIN DE LA CORRECCIÓN ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e60772ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando con Cross-Validation (esto tardará más)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/17 15:30:26 WARN DAGScheduler: Broadcasting large task binary with size 1129.9 KiB\n",
      "25/11/17 15:30:27 WARN DAGScheduler: Broadcasting large task binary with size 1387.5 KiB\n",
      "25/11/17 15:30:29 WARN DAGScheduler: Broadcasting large task binary with size 1674.2 KiB\n",
      "25/11/17 15:30:32 WARN DAGScheduler: Broadcasting large task binary with size 1300.1 KiB\n",
      "25/11/17 15:30:47 WARN DAGScheduler: Broadcasting large task binary with size 1191.7 KiB\n",
      "25/11/17 15:30:48 WARN DAGScheduler: Broadcasting large task binary with size 1466.1 KiB\n",
      "25/11/17 15:30:50 WARN DAGScheduler: Broadcasting large task binary with size 1757.5 KiB\n",
      "25/11/17 15:30:53 WARN DAGScheduler: Broadcasting large task binary with size 1305.1 KiB\n",
      "25/11/17 15:31:10 WARN DAGScheduler: Broadcasting large task binary with size 1265.0 KiB\n",
      "25/11/17 15:31:11 WARN DAGScheduler: Broadcasting large task binary with size 1598.2 KiB\n",
      "25/11/17 15:31:13 WARN DAGScheduler: Broadcasting large task binary with size 1960.0 KiB\n",
      "25/11/17 15:31:17 WARN DAGScheduler: Broadcasting large task binary with size 1418.9 KiB\n",
      "25/11/17 15:31:30 WARN DAGScheduler: Broadcasting large task binary with size 1150.4 KiB\n",
      "25/11/17 15:31:32 WARN DAGScheduler: Broadcasting large task binary with size 1411.9 KiB\n",
      "25/11/17 15:31:33 WARN DAGScheduler: Broadcasting large task binary with size 1692.3 KiB\n",
      "25/11/17 15:31:35 WARN DAGScheduler: Broadcasting large task binary with size 1301.9 KiB\n",
      "25/11/17 15:31:48 WARN DAGScheduler: Broadcasting large task binary with size 1067.8 KiB\n",
      "25/11/17 15:31:49 WARN DAGScheduler: Broadcasting large task binary with size 1428.8 KiB\n",
      "25/11/17 15:31:50 WARN DAGScheduler: Broadcasting large task binary with size 1848.8 KiB\n",
      "25/11/17 15:31:52 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/11/17 15:31:55 WARN DAGScheduler: Broadcasting large task binary with size 1669.3 KiB\n",
      "25/11/17 15:32:12 WARN DAGScheduler: Broadcasting large task binary with size 1334.2 KiB\n",
      "25/11/17 15:32:14 WARN DAGScheduler: Broadcasting large task binary with size 1728.4 KiB\n",
      "25/11/17 15:32:15 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Modelo entrenado y validado!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === PASO 7: Entrenar el Modelo (AHORA SÍ FUNCIONARÁ) ===\n",
    "print(\"\\nEntrenando con Cross-Validation (esto tardará más)...\")\n",
    "cv_model = cv.fit(train_data)\n",
    "\n",
    "print(\"¡Modelo entrenado y validado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f747b472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Realizando predicciones en el conjunto de prueba...\n"
     ]
    }
   ],
   "source": [
    "# === PASO 8: Hacer Predicciones en Datos de PRUEBA ===\n",
    "# Usamos 'cv_model' (que contiene el mejor modelo de los 5)\n",
    "# para predecir en los datos de 'test_data' que NUNCA VIO.\n",
    "print(\"Realizando predicciones en el conjunto de prueba...\")\n",
    "predictions = cv_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95221d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/17 15:32:27 WARN DAGScheduler: Broadcasting large task binary with size 1513.3 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculando Matriz de Confusión Final...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/17 15:32:29 WARN DAGScheduler: Broadcasting large task binary with size 1515.3 KiB\n",
      "25/11/17 15:32:30 WARN DAGScheduler: Broadcasting large task binary with size 1515.3 KiB\n",
      "25/11/17 15:32:32 WARN DAGScheduler: Broadcasting large task binary with size 1515.3 KiB\n",
      "25/11/17 15:32:34 WARN DAGScheduler: Broadcasting large task binary with size 1515.3 KiB\n",
      "[Stage 481:=====================>                                   (3 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Resultados de Evaluación Finales (Post-CV) ---\n",
      "Área bajo la curva PR (en test_data): 0.9952\n",
      "---\n",
      "Verdaderos Positivos (Fraudes Atrapados): 976\n",
      "Verdaderos Negativos (Inocentes Ignorados): 38403\n",
      "---\n",
      "Falsos Positivos (Inocentes Molestados): 366\n",
      "Falsos Negativos (Fraudes Perdidos): 0\n",
      "---\n",
      "Precisión Final: 72.73%\n",
      "Recall Final: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# === PASO 9: Evaluar el Modelo (Resultados Finales) ===\n",
    "# Calculamos la métrica en el conjunto de prueba\n",
    "area_under_pr_final = evaluator.evaluate(predictions)\n",
    "\n",
    "# Calcular la Matriz de Confusión\n",
    "print(\"Calculando Matriz de Confusión Final...\")\n",
    "tp = predictions.filter(\"label == 1 AND prediction == 1\").count()\n",
    "tn = predictions.filter(\"label == 0 AND prediction == 0\").count()\n",
    "fp = predictions.filter(\"label == 0 AND prediction == 1\").count()\n",
    "fn = predictions.filter(\"label == 1 AND prediction == 0\").count()\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "print(f\"\\n--- Resultados de Evaluación Finales (Post-CV) ---\")\n",
    "print(f\"Área bajo la curva PR (en test_data): {area_under_pr_final:.4f}\")\n",
    "print(\"---\")\n",
    "print(f\"Verdaderos Positivos (Fraudes Atrapados): {tp}\")\n",
    "print(f\"Verdaderos Negativos (Inocentes Ignorados): {tn}\")\n",
    "print(\"---\")\n",
    "print(f\"Falsos Positivos (Inocentes Molestados): {fp}\")\n",
    "print(f\"Falsos Negativos (Fraudes Perdidos): {fn}\")\n",
    "print(\"---\")\n",
    "print(f\"Precisión Final: {precision:.2%}\")\n",
    "print(f\"Recall Final: {recall:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98ac721e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando el modelo entrenado en: /home/zidnz/DanaPP/proyecto_fraude/modelos/random_forest_fraude_cv\n",
      "¡Modelo guardado con éxito en /home/zidnz/DanaPP/proyecto_fraude/modelos/random_forest_fraude_cv!\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# === PASO 10: GUARDAR EL MODELO FINAL ===\n",
    "\n",
    "# Define una ruta REAL donde guardarás tu modelo\n",
    "MODEL_SAVE_PATH = \"/home/zidnz/DanaPP/proyecto_fraude/modelos/random_forest_fraude_cv\" \n",
    "\n",
    "print(f\"Guardando el modelo entrenado en: {MODEL_SAVE_PATH}\")\n",
    "\n",
    "# 'cv_model' es la variable que contiene tu modelo entrenado del CrossValidator\n",
    "cv_model.write().overwrite().save(MODEL_SAVE_PATH)\n",
    "\n",
    "print(f\"¡Modelo guardado con éxito en {MODEL_SAVE_PATH}!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mi_proyecto_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
